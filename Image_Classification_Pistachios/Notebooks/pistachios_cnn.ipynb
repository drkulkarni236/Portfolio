{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1. Import Needed Modules](#import) ##\n",
    "## [2. Read in images and create a dataframe of image paths and class labels](#makedf) ## \n",
    "## [3. Trim the train_df dataframe](#trim) ##\n",
    "## [4. Create train, test and validation generators](#generators) ## \n",
    "## [5. Create a function to show Training Image Samples](#show) ## \n",
    "## [6. Create the Model](#model) ## \n",
    "## [7. Create a custom Keras callback to continue or halt training](#callback) ## \n",
    "## [8. Instantiate custom callback and create callbacks to control learning rate and early stopping](#callbacks) ##\n",
    "## [9. Train the model](#train) ##\n",
    "## [10. Define a function to plot the training data](#plot) ##\n",
    "## [11. Make predictions on test set, create Confusion Matrix and Classification Report](#result) ##\n",
    "## [12. Save the model](#save) ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"import\"></a>\n",
    "# <center>Import Need Modules</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T19:26:52.625740Z",
     "iopub.status.busy": "2022-07-29T19:26:52.625101Z",
     "iopub.status.idle": "2022-07-29T19:26:58.893440Z",
     "shell.execute_reply": "2022-07-29T19:26:58.892613Z",
     "shell.execute_reply.started": "2022-07-29T19:26:52.625704Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "# pprevent annoying tensorflow warning\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "import warnings\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', 90)\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"makedf\"></a>\n",
    "# <center>Read in images and create a dataframe of image paths and class labels</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T19:57:30.874105Z",
     "iopub.status.busy": "2022-07-29T19:57:30.873853Z",
     "iopub.status.idle": "2022-07-29T19:57:31.025404Z",
     "shell.execute_reply": "2022-07-29T19:57:31.024675Z",
     "shell.execute_reply.started": "2022-07-29T19:57:30.874077Z"
    }
   },
   "outputs": [],
   "source": [
    "sdir=r'../input/pistachio-image-dataset/Pistachio_Image_Dataset/Pistachio_Image_Dataset'\n",
    "ht=0\n",
    "wt=0\n",
    "samples=0\n",
    "sample_count=10\n",
    "filepaths=[]\n",
    "labels=[]\n",
    "classlist=os.listdir(sdir)\n",
    "for klass in classlist:\n",
    "    classpath=os.path.join(sdir, klass)\n",
    "    if os.path.isdir(classpath):\n",
    "        flist=os.listdir(classpath)    \n",
    "        for i, f in enumerate(flist):\n",
    "            fpath=os.path.join(classpath,f)\n",
    "            filepaths.append(fpath)\n",
    "            labels.append(klass)\n",
    "            #  get image shape to use for averaging of image shapes\n",
    "            if i < sample_count:\n",
    "                img=plt.imread(fpath)               \n",
    "                ht += img.shape[0]\n",
    "                wt += img.shape[1]\n",
    "                samples +=1        \n",
    "Fseries=pd.Series(filepaths, name='filepaths')\n",
    "Lseries=pd.Series(labels, name='labels')\n",
    "df=pd.concat([Fseries, Lseries], axis=1)\n",
    "class_count= len(df['labels'].unique())\n",
    "print('The dataframe has ', class_count, ' classes')\n",
    "# split df into train_df, test_df and valid_df\n",
    "train_df, dummy_df=train_test_split(df, train_size=.8, shuffle=True, random_state=123, stratify=df['labels']) \n",
    "valid_df, test_df = train_test_split(dummy_df, train_size=.5, shuffle=True, random_state=123, stratify=dummy_df['labels'])    \n",
    "print('train_df lenght: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df))\n",
    "classes=list(train_df['labels'].unique())\n",
    "class_count = len(classes)\n",
    "groups=train_df.groupby('labels')\n",
    "print('{0:^30s} {1:^13s}'.format('CLASS', 'IMAGE COUNT'))\n",
    "for label in train_df['labels'].unique():\n",
    "      group=groups.get_group(label)      \n",
    "      print('{0:^30s} {1:^13s}'.format(label, str(len(group))))\n",
    "wave=wt/samples\n",
    "have=ht/samples\n",
    "aspect_ratio= have/wave\n",
    "print ('Average Image Height: ' ,have, '  Average Image Width: ', wave, '  Aspect ratio: ', aspect_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset is well balanced but has more images per class than is necessary. Limit the number of images in each class\n",
    "### to 500 using the trim function defined below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trim\"></a>\n",
    "# <center>Trim the df dataframe to max and min samples</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T19:40:34.422248Z",
     "iopub.status.busy": "2022-07-29T19:40:34.421574Z",
     "iopub.status.idle": "2022-07-29T19:40:34.433305Z",
     "shell.execute_reply": "2022-07-29T19:40:34.432521Z",
     "shell.execute_reply.started": "2022-07-29T19:40:34.422213Z"
    }
   },
   "outputs": [],
   "source": [
    "def trim (df, max_size, min_size, column):\n",
    "    df=df.copy()\n",
    "    original_class_count= len(list(df[column].unique()))    \n",
    "    sample_list=[] \n",
    "    groups=df.groupby(column)\n",
    "    for label in df[column].unique():        \n",
    "        group=groups.get_group(label)\n",
    "        sample_count=len(group)         \n",
    "        if sample_count> max_size :\n",
    "            strat=group[column]\n",
    "            samples,_=train_test_split(group, train_size=max_size, shuffle=True, random_state=123, stratify=strat)            \n",
    "            sample_list.append(samples)\n",
    "        elif sample_count>= min_size:\n",
    "            sample_list.append(group)\n",
    "    df=pd.concat(sample_list, axis=0).reset_index(drop=True)\n",
    "    final_class_count= len(list(df[column].unique())) \n",
    "    if final_class_count != original_class_count:\n",
    "        print ('*** WARNING***  dataframe has a reduced number of classes from ', original_class_count,' to ', final_class_count )\n",
    "    groups=df.groupby('labels')\n",
    "    print('{0:^30s} {1:^13s}'.format('CLASS', 'IMAGE COUNT'))\n",
    "    for label in train_df['labels'].unique():\n",
    "          group=groups.get_group(label)      \n",
    "          print('{0:^30s} {1:^13s}'.format(label, str(len(group))))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T19:40:38.469417Z",
     "iopub.status.busy": "2022-07-29T19:40:38.468905Z",
     "iopub.status.idle": "2022-07-29T19:40:38.487014Z",
     "shell.execute_reply": "2022-07-29T19:40:38.486232Z",
     "shell.execute_reply.started": "2022-07-29T19:40:38.469382Z"
    }
   },
   "outputs": [],
   "source": [
    "max_samples=500\n",
    "min_samples=0\n",
    "column='labels'\n",
    "train_df=trim(train_df, max_samples, min_samples, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:03:01.902249Z",
     "iopub.status.busy": "2022-07-29T20:03:01.901930Z",
     "iopub.status.idle": "2022-07-29T20:03:01.996659Z",
     "shell.execute_reply": "2022-07-29T20:03:01.995958Z",
     "shell.execute_reply.started": "2022-07-29T20:03:01.902208Z"
    }
   },
   "outputs": [],
   "source": [
    "# load in our data\n",
    "our_df = pd.read_csv(\"../input/train-val-test-pistachios/pistachios.csv\", index_col=0)\n",
    "df = pd.DataFrame()\n",
    "df[\"labels\"] = our_df[\"pistachio_type\"].map({0: \"Kirmizi_Pistachio\", 1: \"Siirt_Pistachio\"})\n",
    "df[\"filepaths\"] = our_df[\"filename\"]\n",
    "base = \"../input/pistachio-image-dataset/Pistachio_Image_Dataset/Pistachio_Image_Dataset\"\n",
    "df.loc[df.filepaths.str.contains(\"kirmizi\"), \"filepaths\"] = \"{}/Kirmizi_Pistachio/\".format(base) + df.loc[df.filepaths.str.contains(\"kirmizi\"), \"filepaths\"]\n",
    "df.loc[df.filepaths.str.contains(\"siirt\"), \"filepaths\"] = \"{}/Siirt_Pistachio/\".format(base) + df.loc[df.filepaths.str.contains(\"siirt\"), \"filepaths\"]\n",
    "print(sum(df.filepaths.str.contains(\"\\(\")))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:03:04.558944Z",
     "iopub.status.busy": "2022-07-29T20:03:04.558365Z",
     "iopub.status.idle": "2022-07-29T20:03:04.582686Z",
     "shell.execute_reply": "2022-07-29T20:03:04.581826Z",
     "shell.execute_reply.started": "2022-07-29T20:03:04.558904Z"
    }
   },
   "outputs": [],
   "source": [
    "# use our specific train, val, and test dataframes\n",
    "train_labels = pd.read_csv(\"../input/train-val-test-pistachios/pistachios_with_cnn_train_labels.csv\", index_col=0)\n",
    "train_df = df.loc[train_labels.index]\n",
    "val_labels = pd.read_csv(\"../input/train-val-test-pistachios/pistachios_with_cnn_val_labels.csv\", index_col=0)\n",
    "valid_df = df.loc[val_labels.index]\n",
    "test_labels = pd.read_csv(\"../input/train-val-test-pistachios/pistachios_with_cnn_test_labels.csv\", index_col=0)\n",
    "test_df = df.loc[test_labels.index]\n",
    "train_df.labels.value_counts(), valid_df.labels.value_counts(), test_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"generators\"></a>\n",
    "# <center>Create the train_gen, test_gen and valid_gen</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:03:14.292898Z",
     "iopub.status.busy": "2022-07-29T20:03:14.292418Z",
     "iopub.status.idle": "2022-07-29T20:03:17.264784Z",
     "shell.execute_reply": "2022-07-29T20:03:17.263455Z",
     "shell.execute_reply.started": "2022-07-29T20:03:14.292859Z"
    }
   },
   "outputs": [],
   "source": [
    "working_dir=r'./'\n",
    "img_size=(300,300)\n",
    "batch_size=25 # We will use and EfficientetB3 model, with image size of (200, 250) this size should not cause resource error\n",
    "trgen=ImageDataGenerator(horizontal_flip=True,rotation_range=20, width_shift_range=.2,\n",
    "                                  height_shift_range=.2, zoom_range=.2 )\n",
    "t_and_v_gen=ImageDataGenerator()\n",
    "# train_gen=trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "#                                    class_mode='categorical', color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
    "# valid_gen=t_and_v_gen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "#                                    class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=batch_size)\n",
    "train_gen=trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                   class_mode='sparse', color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
    "valid_gen=t_and_v_gen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                   class_mode='sparse', color_mode='rgb', shuffle=False, batch_size=batch_size)\n",
    "# for the test_gen we want to calculate the batch size and test steps such that batch_size X test_steps= number of samples in test set\n",
    "# this insures that we go through all the sample in the test set exactly once.\n",
    "length=len(test_df)\n",
    "test_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]  \n",
    "test_steps=int(length/test_batch_size)\n",
    "# test_gen=t_and_v_gen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "#                                    class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n",
    "test_gen=t_and_v_gen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                   class_mode='sparse', color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n",
    "# from the generator we can get information we will need later\n",
    "classes=list(train_gen.class_indices.keys())\n",
    "class_indices=list(train_gen.class_indices.values())\n",
    "class_count=len(classes)\n",
    "labels=test_gen.labels\n",
    "print ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps, ' number of classes : ', class_count)\n",
    "print ('{0:^25s}{1:^12s}'.format('class name', 'class index'))\n",
    "for klass, index in zip(classes, class_indices):\n",
    "    print(f'{klass:^25s}{str(index):^12s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"show\"></a>\n",
    "# <center>Create a function to show example training images</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:03:29.330269Z",
     "iopub.status.busy": "2022-07-29T20:03:29.329639Z",
     "iopub.status.idle": "2022-07-29T20:03:32.852814Z",
     "shell.execute_reply": "2022-07-29T20:03:32.852180Z",
     "shell.execute_reply.started": "2022-07-29T20:03:29.330233Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_image_samples(gen ):\n",
    "    t_dict=gen.class_indices\n",
    "    classes=list(t_dict.keys())    \n",
    "    images,labels=next(gen) # get a sample batch from the generator \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    length=len(labels)\n",
    "    if length<25:   #show maximum of 25 images\n",
    "        r=length\n",
    "    else:\n",
    "        r=25\n",
    "    for i in range(r):        \n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        image=images[i] /255       \n",
    "        plt.imshow(image)\n",
    "        index=np.argmax(labels[i])\n",
    "        class_name=classes[index]\n",
    "        plt.title(class_name, color='blue', fontsize=12)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "show_image_samples(train_gen )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "# <center>Create a model using transfer learning with EfficientNetB3</center>\n",
    "### NOTE experts advise you make the base model initially not trainable. Then train for some number of epochs\n",
    "### then fine tune model by making base model trainable and run more epochs\n",
    "### I have found this to be WRONG!!!!\n",
    "### Making the base model trainable from the outset leads to faster convegence and a lower validation loss\n",
    "### for the same number of total epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:03:38.961198Z",
     "iopub.status.busy": "2022-07-29T20:03:38.960511Z",
     "iopub.status.idle": "2022-07-29T20:03:44.747022Z",
     "shell.execute_reply": "2022-07-29T20:03:44.746222Z",
     "shell.execute_reply.started": "2022-07-29T20:03:38.961160Z"
    }
   },
   "outputs": [],
   "source": [
    "img_shape=(img_size[0], img_size[1], 3)\n",
    "model_name='EfficientNetB3'\n",
    "base_model=tf.keras.applications.efficientnet.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \n",
    "# Note you are always told NOT to make the base model trainable initially- that is WRONG you get better results leaving it trainable\n",
    "base_model.trainable=True\n",
    "x=base_model.output\n",
    "x=BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n",
    "x = Dense(128, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n",
    "                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n",
    "x=Dropout(rate=.3, seed=123)(x)\n",
    "x = Dense(8, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n",
    "                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n",
    "x=Dropout(rate=.45, seed=123)(x)        \n",
    "output=Dense(class_count, activation='softmax')(x)\n",
    "model=Model(inputs=base_model.input, outputs=output)\n",
    "lr=.001 # start with this learning rate\n",
    "# model.compile(Adamax(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "model.compile(Adamax(learning_rate=lr), loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"callback\"></a>\n",
    "# <center>Create a custom Keras callback to continue or halt training</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:03:44.748965Z",
     "iopub.status.busy": "2022-07-29T20:03:44.748711Z",
     "iopub.status.idle": "2022-07-29T20:03:44.763637Z",
     "shell.execute_reply": "2022-07-29T20:03:44.762630Z",
     "shell.execute_reply.started": "2022-07-29T20:03:44.748922Z"
    }
   },
   "outputs": [],
   "source": [
    "class ASK(keras.callbacks.Callback):\n",
    "    def __init__ (self, model, epochs,  ask_epoch): # initialization of the callback\n",
    "        super(ASK, self).__init__()\n",
    "        self.model=model               \n",
    "        self.ask_epoch=ask_epoch\n",
    "        self.epochs=epochs\n",
    "        self.ask=True # if True query the user on a specified epoch\n",
    "        \n",
    "    def on_train_begin(self, logs=None): # this runs on the beginning of training\n",
    "        if self.ask_epoch == 0: \n",
    "            print('you set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n",
    "            self.ask_epoch=1\n",
    "        if self.ask_epoch >= self.epochs: # you are running for epochs but ask_epoch>epochs\n",
    "            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n",
    "            self.ask=False # do not query the user\n",
    "        if self.epochs == 1:\n",
    "            self.ask=False # running only for 1 epoch so do not query user\n",
    "        else:\n",
    "            print('Training will proceed until epoch', ask_epoch,' then you will be asked to') \n",
    "            print(' enter H to halt training or enter an integer for how many more epochs to run then be asked again')  \n",
    "        self.start_time= time.time() # set the time at which training started\n",
    "        \n",
    "    def on_train_end(self, logs=None):   # runs at the end of training     \n",
    "        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted         \n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "        print (msg, flush=True) # print out training duration time\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n",
    "        if self.ask: # are the conditions right to query the user?\n",
    "            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?\n",
    "                print('\\n Enter H to end training or  an integer for the number of additional epochs to run then ask again')\n",
    "                ans=input()\n",
    "                \n",
    "                if ans == 'H' or ans =='h' or ans == '0': # quit training for these conditions\n",
    "                    print ('you entered ', ans, ' Training halted on epoch ', epoch+1, ' due to user input\\n', flush=True)\n",
    "                    self.model.stop_training = True # halt training\n",
    "                else: # user wants to continue training\n",
    "                    self.ask_epoch += int(ans)\n",
    "                    if self.ask_epoch > self.epochs:\n",
    "                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush =True)\n",
    "                    else:\n",
    "                        print ('you entered ', ans, ' Training will continue to epoch ', self.ask_epoch, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"callbacks\"></a>\n",
    "# <center>Instantiate custom callback and create 2 callbacks to control learning rate and early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:03:45.825037Z",
     "iopub.status.busy": "2022-07-29T20:03:45.824482Z",
     "iopub.status.idle": "2022-07-29T20:03:45.830926Z",
     "shell.execute_reply": "2022-07-29T20:03:45.829845Z",
     "shell.execute_reply.started": "2022-07-29T20:03:45.824999Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs=40\n",
    "ask_epoch=40\n",
    "ask=ASK(model, epochs,  ask_epoch)\n",
    "rlronp=tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2,verbose=1)\n",
    "estop=tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, verbose=1,restore_best_weights=True)\n",
    "callbacks=[rlronp, estop, ask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "# <center>Train the model\n",
    "### Note unlike how you are told it is BETTER to make the base model trainable from the outset\n",
    "### It will converge faster and have a lower validation losss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:03:48.598339Z",
     "iopub.status.busy": "2022-07-29T20:03:48.597517Z",
     "iopub.status.idle": "2022-07-29T20:35:15.290363Z",
     "shell.execute_reply": "2022-07-29T20:35:15.289576Z",
     "shell.execute_reply.started": "2022-07-29T20:03:48.598288Z"
    }
   },
   "outputs": [],
   "source": [
    "history=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n",
    "               validation_steps=None,  shuffle=False,  initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot\"></a>\n",
    "# <center>Define a function to plot the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:35:15.292414Z",
     "iopub.status.busy": "2022-07-29T20:35:15.292157Z",
     "iopub.status.idle": "2022-07-29T20:35:15.830102Z",
     "shell.execute_reply": "2022-07-29T20:35:15.829342Z",
     "shell.execute_reply.started": "2022-07-29T20:35:15.292379Z"
    }
   },
   "outputs": [],
   "source": [
    "# def tr_plot(tr_data, start_epoch):\n",
    "#     #Plot the training and validation data\n",
    "#     tacc=tr_data.history['accuracy']\n",
    "#     tloss=tr_data.history['loss']\n",
    "#     vacc=tr_data.history['val_accuracy']\n",
    "#     vloss=tr_data.history['val_loss']\n",
    "#     Epoch_count=len(tacc)+ start_epoch\n",
    "#     Epochs=[]\n",
    "#     for i in range (start_epoch ,Epoch_count):\n",
    "#         Epochs.append(i+1)   \n",
    "#     index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n",
    "#     val_lowest=vloss[index_loss]\n",
    "#     index_acc=np.argmax(vacc)\n",
    "#     acc_highest=vacc[index_acc]\n",
    "#     plt.style.use('fivethirtyeight')\n",
    "#     sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n",
    "#     vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n",
    "#     fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n",
    "#     axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n",
    "#     axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n",
    "#     axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n",
    "#     axes[0].set_title('Training and Validation Loss')\n",
    "#     axes[0].set_xlabel('Epochs')\n",
    "#     axes[0].set_ylabel('Loss')\n",
    "#     axes[0].legend()\n",
    "#     axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n",
    "#     axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n",
    "#     axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n",
    "#     axes[1].set_title('Training and Validation Accuracy')\n",
    "#     axes[1].set_xlabel('Epochs')\n",
    "#     axes[1].set_ylabel('Accuracy')\n",
    "#     axes[1].legend()\n",
    "#     plt.tight_layout    \n",
    "#     plt.show()\n",
    "    \n",
    "# tr_plot(history,0)\n",
    "def tr_plot(tr_data, start_epoch):\n",
    "    #Plot the training and validation data\n",
    "    tacc=tr_data.history['sparse_categorical_accuracy']\n",
    "    tloss=tr_data.history['loss']\n",
    "    vacc=tr_data.history['val_sparse_categorical_accuracy']\n",
    "    vloss=tr_data.history['val_loss']\n",
    "    Epoch_count=len(tacc)+ start_epoch\n",
    "    Epochs=[]\n",
    "    for i in range (start_epoch ,Epoch_count):\n",
    "        Epochs.append(i+1)   \n",
    "    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n",
    "    val_lowest=vloss[index_loss]\n",
    "    index_acc=np.argmax(vacc)\n",
    "    acc_highest=vacc[index_acc]\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n",
    "    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n",
    "    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n",
    "    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n",
    "    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n",
    "    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n",
    "    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n",
    "    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout    \n",
    "    plt.show()\n",
    "    \n",
    "tr_plot(history,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"result\"></a>\n",
    "# <center>Make predictions on test set, create Confusion Matrix and Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:35:15.831824Z",
     "iopub.status.busy": "2022-07-29T20:35:15.831545Z",
     "iopub.status.idle": "2022-07-29T20:35:21.255078Z",
     "shell.execute_reply": "2022-07-29T20:35:21.254335Z",
     "shell.execute_reply.started": "2022-07-29T20:35:15.831789Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred= []\n",
    "y_true=test_gen.labels\n",
    "errors=0\n",
    "preds=model.predict(test_gen, steps=test_steps, verbose=1) # predict on the test set\n",
    "tests=len(preds)\n",
    "for i, p in enumerate(preds):\n",
    "        pred_index=np.argmax(p)         \n",
    "        true_index=test_gen.labels[i]  # labels are integer values\n",
    "        if pred_index != true_index: # a misclassification has occurred                                           \n",
    "            errors=errors + 1\n",
    "        y_pred.append(pred_index)\n",
    "acc=( 1-errors/tests) * 100\n",
    "print(f'there were {errors} in {tests} tests for an accuracy of {acc:6.2f}')\n",
    "ypred=np.array(y_pred)\n",
    "ytrue=np.array(y_true)\n",
    "cm = confusion_matrix(ytrue, ypred )\n",
    "# plot the confusion matrix\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n",
    "plt.xticks(np.arange(class_count)+.5, classes, rotation=90)\n",
    "plt.yticks(np.arange(class_count)+.5, classes, rotation=0)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "clr = classification_report(y_true, y_pred, target_names=classes, digits= 4) # create classification report\n",
    "print(\"Classification Report:\\n----------------------\\n\", clr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "# <center>Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:45:07.282975Z",
     "iopub.status.busy": "2022-07-29T20:45:07.282475Z",
     "iopub.status.idle": "2022-07-29T20:45:08.549306Z",
     "shell.execute_reply": "2022-07-29T20:45:08.547871Z",
     "shell.execute_reply.started": "2022-07-29T20:45:07.282939Z"
    }
   },
   "outputs": [],
   "source": [
    "subject='pistachios' \n",
    "acc=str(( 1-errors/tests) * 100)\n",
    "index=acc.rfind('.')\n",
    "acc=acc[:index + 3]\n",
    "save_id= subject + '_' + str(acc) + '.h5' \n",
    "model_save_loc=os.path.join(working_dir, save_id)\n",
    "model.save(model_save_loc)\n",
    "print ('model was saved as ' , model_save_loc ) \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the second to last layer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:49:10.671230Z",
     "iopub.status.busy": "2022-07-29T20:49:10.670966Z",
     "iopub.status.idle": "2022-07-29T20:49:10.677819Z",
     "shell.execute_reply": "2022-07-29T20:49:10.677052Z",
     "shell.execute_reply.started": "2022-07-29T20:49:10.671202Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df), batch_size, int(np.ceil(len(df)/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:49:10.759776Z",
     "iopub.status.busy": "2022-07-29T20:49:10.759233Z",
     "iopub.status.idle": "2022-07-29T20:49:10.788768Z",
     "shell.execute_reply": "2022-07-29T20:49:10.788157Z",
     "shell.execute_reply.started": "2022-07-29T20:49:10.759747Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# with a Sequential model\n",
    "get_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[-2].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:49:10.810395Z",
     "iopub.status.busy": "2022-07-29T20:49:10.810199Z",
     "iopub.status.idle": "2022-07-29T20:49:11.648974Z",
     "shell.execute_reply": "2022-07-29T20:49:11.648164Z",
     "shell.execute_reply.started": "2022-07-29T20:49:10.810372Z"
    }
   },
   "outputs": [],
   "source": [
    "gen=t_and_v_gen.flow_from_dataframe(df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "                                   class_mode='sparse', color_mode='rgb', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:49:11.651177Z",
     "iopub.status.busy": "2022-07-29T20:49:11.650712Z",
     "iopub.status.idle": "2022-07-29T20:49:34.456268Z",
     "shell.execute_reply": "2022-07-29T20:49:34.455532Z",
     "shell.execute_reply.started": "2022-07-29T20:49:11.651138Z"
    }
   },
   "outputs": [],
   "source": [
    "steps = int(np.ceil(len(df)/batch_size))\n",
    "gen.reset()\n",
    "labels = np.array([])\n",
    "outputs = None\n",
    "for i in range(steps):\n",
    "    current, current_labels = gen.next()\n",
    "    labels = np.concatenate([labels, current_labels])\n",
    "    print(i, current.mean())\n",
    "    current_output = get_layer_output(current)\n",
    "    if outputs is None:\n",
    "        outputs = current_output[0]\n",
    "    else:\n",
    "        outputs = np.concatenate([outputs, current_output[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:49:34.457833Z",
     "iopub.status.busy": "2022-07-29T20:49:34.457542Z",
     "iopub.status.idle": "2022-07-29T20:49:34.463888Z",
     "shell.execute_reply": "2022-07-29T20:49:34.462972Z",
     "shell.execute_reply.started": "2022-07-29T20:49:34.457792Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:49:34.467600Z",
     "iopub.status.busy": "2022-07-29T20:49:34.467194Z",
     "iopub.status.idle": "2022-07-29T20:49:34.495125Z",
     "shell.execute_reply": "2022-07-29T20:49:34.494445Z",
     "shell.execute_reply.started": "2022-07-29T20:49:34.467564Z"
    }
   },
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(outputs)\n",
    "out_df.to_csv(\"second_last_layer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:49:34.496669Z",
     "iopub.status.busy": "2022-07-29T20:49:34.496291Z",
     "iopub.status.idle": "2022-07-29T20:49:34.516527Z",
     "shell.execute_reply": "2022-07-29T20:49:34.515737Z",
     "shell.execute_reply.started": "2022-07-29T20:49:34.496633Z"
    }
   },
   "outputs": [],
   "source": [
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T20:49:34.519980Z",
     "iopub.status.busy": "2022-07-29T20:49:34.518116Z",
     "iopub.status.idle": "2022-07-29T20:49:34.527934Z",
     "shell.execute_reply": "2022-07-29T20:49:34.527222Z",
     "shell.execute_reply.started": "2022-07-29T20:49:34.519941Z"
    }
   },
   "outputs": [],
   "source": [
    "model.layers[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
